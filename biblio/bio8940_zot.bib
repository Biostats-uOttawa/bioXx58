@article{alston_beginner_2021,
  title = {A {{Beginner}}'s {{Guide}} to {{Conducting Reproducible Research}}},
  author = {Alston, Jesse M. and Rick, Jessica A.},
  year = {2021},
  journal = {The Bulletin of the Ecological Society of America},
  volume = {102},
  number = {2},
  pages = {e01801},
  issn = {2327-6096},
  doi = {10.1002/bes2.1801},
  urldate = {2022-09-02},
  langid = {english},
  file = {/home/julien/Documents/PDF/Alston_Rick_2021_The Bulletin of the Ecological Society of America.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/252Y3VAD/bes2.html}
}

@article{bolker_generalized_2009,
  title = {Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution},
  author = {Bolker, Benjamin M and Brooks, Mollie E and Clark, Connie J and Geange, Shane W and Poulsen, John R and Stevens, M Henry H and White, Jada-Simone S},
  year = {2009},
  month = mar,
  journal = {Trends in Ecology and Evolution},
  volume = {24},
  number = {3},
  pages = {127--135},
  abstract = {How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize [`]best-practice' data analysis procedures for scientists facing this challenge},
  annotation = {Published: Journal},
  file = {/home/julien/Documents/PDF/Bolker_et_al_2009_Trends_in_Ecology_&_Evolution.pdf}
}

@article{cinelli_crash_2022,
  title = {A {{Crash Course}} in {{Good}} and {{Bad Controls}}},
  author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
  year = {2022},
  month = may,
  journal = {Sociological Methods \& Research},
  pages = {00491241221099552},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/00491241221099552},
  urldate = {2024-01-04},
  abstract = {Many students of statistics and econometrics express frustration with the way a problem known as ``bad control'' is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this ``crash course'' accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models.},
  langid = {english},
  file = {/home/jmartin/Documents/PDF/Cinelli et al_2022_Sociological Methods & Research.pdf}
}

@article{etz_how_2018,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Etz, Alexander and Gronau, Quentin F. and Dablander, Fabian and Edelsbrunner, Peter A. and Baribault, Beth},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {219--234},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1317-5},
  urldate = {2021-01-25},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to gain background knowledge about various theoretical specifics and Bayesian approaches to frequently used models. Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment. After consulting our guide, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.},
  langid = {english},
  file = {/home/julien/Documents/PDF/Etz et al_2018_Psychonomic Bulletin & Review.pdf}
}

@article{forstmeier_detecting_2017,
  title = {Detecting and Avoiding Likely False-Positive Findings -- a Practical Guide},
  author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
  year = {2017},
  journal = {Biological Reviews},
  volume = {92},
  number = {4},
  pages = {1941--1968},
  issn = {1469-185X},
  doi = {10.1111/brv.12315},
  urldate = {2023-03-04},
  abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of `you can publish if you found a significant effect'. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of `you can publish if your study is rigorous'. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of `impact' almost exclusively and towards a system which explicitly values indices of scientific rigour.},
  langid = {english},
  keywords = {confirmation bias,HARKing,hindsight bias,overfitting,P-hacking,power,preregistration,replication,researcher degrees of freedom,Type I error},
  file = {/home/julien/Documents/PDF/bibliobdd/zotero/storage/U5TZCJ9K/Forstmeier et al. - 2017 - Detecting and avoiding likely false-positive findi.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/H265D8HE/brv.html}
}

@article{greenland_statistical_2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  urldate = {2021-03-29},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so---and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english},
  file = {/home/julien/Documents/PDF/Greenland et al_2016_European Journal of Epidemiology.pdf}
}

@article{harrison_brief_2018,
  title = {A Brief Introduction to Mixed Effects Modelling and Multi-Model Inference in Ecology},
  author = {Harrison, Xavier A. and Donaldson, Lynda and {Correa-Cano}, Maria Eugenia and Evans, Julian and Fisher, David N. and Goodwin, Cecily E. D. and Robinson, Beth S. and Hodgson, David J. and Inger, Richard},
  year = {2018},
  month = may,
  journal = {PeerJ},
  volume = {6},
  pages = {e4794},
  issn = {2167-8359},
  doi = {10.7717/peerj.4794},
  urldate = {2018-05-24},
  abstract = {The use of linear mixed effects models (LMMs) is increasingly common in the analysis of biological data. Whilst LMMs offer a flexible approach to modelling a broad range of data types, ecological data are often complex and require complex model structures, and the fitting and interpretation of such models is not always straightforward. The ability to achieve robust biological inference requires that practitioners know how and when to apply these tools. Here, we provide a general overview of current methods for the application of LMMs to biological data, and highlight the typical pitfalls that can be encountered in the statistical modelling process. We tackle several issues regarding methods of model selection, with particular reference to the use of information theory and multi-model inference in ecology. We offer practical solutions and direct the reader to key references that provide further technical detail for those seeking a deeper understanding. This overview should serve as a widely accessible code of best practice for applying LMMs to complex biological problems and model structures, and in doing so improve the robustness of conclusions drawn from studies investigating ecological and evolutionary questions.},
  langid = {english},
  file = {/home/julien/Documents/PDF/Harrison_et_al_2018_PeerJ.pdf}
}

@article{ivimey-cook_implementing_2023,
  title = {Implementing Code Review in the Scientific Workflow: {{Insights}} from Ecology and Evolutionary Biology},
  shorttitle = {Implementing Code Review in the Scientific Workflow},
  author = {{Ivimey-Cook}, Edward R. and Pick, Joel L. and {Bairos-Novak}, Kevin R. and Culina, Antica and Gould, Elliot and Grainger, Matthew and Marshall, Benjamin M. and Moreau, David and Paquet, Matthieu and Royaut{\'e}, Rapha{\"e}l and {S{\'a}nchez-T{\'o}jar}, Alfredo and Silva, In{\^e}s and Windecker, Saras M.},
  year = {2023},
  journal = {Journal of Evolutionary Biology},
  volume = {36},
  number = {10},
  pages = {1347--1356},
  issn = {1420-9101},
  doi = {10.1111/jeb.14230},
  urldate = {2024-01-04},
  abstract = {Code review increases reliability and improves reproducibility of research. As such, code review is an inevitable step in software development and is common in fields such as computer science. However, despite its importance, code review is noticeably lacking in ecology and evolutionary biology. This is problematic as it facilitates the propagation of coding errors and a reduction in reproducibility and reliability of published results. To address this, we provide a detailed commentary on how to effectively review code, how to set up your project to enable this form of review and detail its possible implementation at several stages throughout the research process. This guide serves as a primer for code review, and adoption of the principles and advice here will go a long way in promoting more open, reliable, and transparent ecology and evolutionary biology.},
  copyright = {{\copyright} 2023 The Authors. Journal of Evolutionary Biology published by John Wiley \& Sons Ltd on behalf of European Society for Evolutionary Biology.},
  langid = {english},
  keywords = {coding errors,open science,reliability,reproducibility,research process,software development,transparency},
  file = {/home/jmartin/Documents/PDF/Ivimey-Cook et al_2023_Journal of Evolutionary Biology.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/N8VGFMZV/jeb.html}
}

@article{kruschke_bayesian_2018,
  title = {Bayesian Data Analysis for Newcomers},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {155--177},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1272-1},
  urldate = {2021-01-25},
  abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
  langid = {english},
  file = {/home/julien/Documents/PDF/Kruschke_Liddell_2018_Psychonomic Bulletin & Review.pdf}
}

@article{kuffner_why_2019,
  title = {Why Are P-{{Values Controversial}}?},
  author = {Kuffner, Todd A. and Walker, Stephen G.},
  year = {2019},
  month = jan,
  journal = {The American Statistician},
  volume = {73},
  number = {1},
  pages = {1--3},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1277161},
  urldate = {2020-12-08},
  abstract = {While it is often argued that a p-value is a probability; see Wasserstein and Lazar, we argue that a p-value is not defined as a probability. A p-value is a bijection of the sufficient statistic for a given test which maps to the same scale as the Type I error probability. As such, the use of p-values in a test should be no more a source of controversy than the use of a sufficient statistic. It is demonstrated that there is, in fact, no ambiguity about what a p-value is, contrary to what has been claimed in recent public debates in the applied statistics community. We give a simple example to illustrate that rejecting the use of p-values in testing for a normal mean parameter is conceptually no different from rejecting the use of a sample mean. The p-value is innocent; the problem arises from its misuse and misinterpretation. The way that p-values have been informally defined and interpreted appears to have led to tremendous confusion and controversy regarding their place in statistical analysis.},
  keywords = {Decision rule,Sufficient statistic,Type I error},
  file = {/home/julien/Documents/PDF/Kuffner_Walker_2019_The American Statistician.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/Q4XXW99S/00031305.2016.html}
}

@article{schielzeth_conclusions_2009,
  title = {Conclusions beyond Support: Overconfident Estimates in Mixed Models},
  shorttitle = {Conclusions beyond Support},
  author = {Schielzeth, Holger and Forstmeier, Wolfgang},
  year = {2009},
  journal = {Behavioral Ecology},
  volume = {20},
  number = {2},
  pages = {416--420},
  issn = {1045-2249},
  doi = {10.1093/beheco/arn145},
  urldate = {2020-12-11},
  abstract = {Mixed-effect models are frequently used to control for the nonindependence of data points, for example, when repeated measures from the same individuals are available. The aim of these models is often to estimate fixed effects and to test their significance. This is usually done by including random intercepts, that is, intercepts that are allowed to vary between individuals. The widespread belief is that this controls for all types of pseudoreplication within individuals. Here we show that this is not the case, if the aim is to estimate effects that vary within individuals and individuals differ in their response to these effects. In these cases, random intercept models give overconfident estimates leading to conclusions that are not supported by the data. By allowing individuals to differ in the slopes of their responses, it is possible to account for the nonindependence of data points that pseudoreplicate slope information. Such random slope models give appropriate standard errors and are easily implemented in standard statistical software. Because random slope models are not always used where they are essential, we suspect that many published findings have too narrow confidence intervals and a substantially inflated type I error rate. Besides reducing type I errors, random slope models have the potential to reduce residual variance by accounting for between-individual variation in slopes, which makes it easier to detect treatment effects that are applied between individuals, hence reducing type II errors as well.},
  pmcid = {PMC2657178},
  pmid = {19461866},
  file = {/home/julien/Documents/PDF/bibliobdd/zotero/storage/ZNGJFF7R/Schielzeth and Forstmeier - 2009 - Conclusions beyond support overconfident estimate.pdf}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  publisher = {Royal Society},
  doi = {10.1098/rsos.160384},
  urldate = {2020-12-08},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  file = {/home/julien/Documents/PDF/Smaldino_McElreath_Royal Society Open Science.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/9WVBNII3/rsos.html}
}

@article{trafimow_five_2019,
  title = {Five {{Nonobvious Changes}} in {{Editorial Practice}} for {{Editors}} and {{Reviewers}} to {{Consider When Evaluating Submissions}} in a {{Post}} p {$<$} 0.05 {{Universe}}},
  author = {Trafimow, David},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {340--345},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537888},
  urldate = {2020-12-08},
  abstract = {The American Statistical Association's Symposium on Statistical Inference (SSI) included a session on how editorial practices should change in a universe no longer dominated by null hypothesis significance testing (NHST). The underlying assumptions were first, that NHST is problematic; and second, that editorial practices really should change. The present article is based on my talk in this session, and on these assumptions. Consistent with the spirit of the SSI, my focus is not on what reviewers and editors should not do (e.g., NHST) but rather on what they should do, with an emphasis on changes that are not obvious. The recommended changes include a wider consideration of the nature of the contribution than submitted manuscripts usually receive; a greater tolerance of ambiguity; more of an emphasis on the thinking and execution of the study, with a decreased emphasis on the findings; replacing NHST with the a priori procedure; and a call for reviewers and editors to recognize that there are many cases where the basic assumptions of inferential statistical procedures simply are not met, and that inferential statistics (even the a priori procedure) may consequently be inappropriate.},
  keywords = {A priori procedure,Editorial changes,Editorial practices,Journal practices},
  file = {/home/julien/Documents/PDF/Trafimow_2019_The American Statistician.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/786BPDBP/00031305.2018.html}
}

@article{vandepol2009,
  title = {A Simple Method for Distinguishing Within-versus between-Subject Effects Using Mixed Models},
  author = {{Van de Pol}, Martijn and Wright, Jonathan},
  year = {2009},
  journal = {Animal behaviour},
  volume = {77},
  number = {3},
  pages = {753--758},
  file = {/home/julien/Documents/PDF/Van_de_Pol_Wright_2009_Animal_behaviour.pdf}
}

@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2021-03-29},
  file = {/home/julien/Documents/PDF/Wasserstein_Lazar_2016_The American Statistician.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/SIKTN7U4/00031305.2016.html}
}

@book{wickham_data_2016,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  month = dec,
  edition = {1st edition},
  publisher = {O'Reilly Media},
  abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.You'll learn how to:Wrangle---transform your datasets into a form convenient for analysisProgram---learn powerful R tools for solving data problems with greater clarity and easeExplore---examine your data, generate hypotheses, and quickly test themModel---provide a low-dimensional summary that captures true "signals" in your datasetCommunicate---learn R Markdown for integrating prose, code, and results},
  langid = {english}
}

@article{wysocki_statistical_2022,
  title = {Statistical {{Control Requires Causal Justification}}},
  author = {Wysocki, Anna C. and Lawson, Katherine M. and Rhemtulla, Mijke},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {25152459221095823},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459221095823},
  urldate = {2023-06-02},
  abstract = {It is common practice in correlational or quasiexperimental studies to use statistical control to remove confounding effects from a regression coefficient. Controlling for relevant confounders can debias the estimated causal effect of a predictor on an outcome; that is, it can bring the estimated regression coefficient closer to the value of the true causal effect. But statistical control works only under ideal circumstances. When the selected control variables are inappropriate, controlling can result in estimates that are more biased than uncontrolled estimates. Despite the ubiquity of statistical control in published regression analyses and the consequences of controlling for inappropriate third variables, the selection of control variables is rarely explicitly justified in print. We argue that to carefully select appropriate control variables, researchers must propose and defend a causal structure that includes the outcome, predictors, and plausible confounders. We underscore the importance of causality when selecting control variables by demonstrating how regression coefficients are affected by controlling for appropriate and inappropriate variables. Finally, we provide practical recommendations for applied researchers who wish to use statistical control.},
  langid = {english},
  file = {/home/julien/Documents/PDF/Wysocki et al_2022_Advances in Methods and Practices in Psychological Science.pdf}
}

@article{zuur_protocol_2010,
  title = {A Protocol for Data Exploration to Avoid Common Statistical Problems},
  author = {Zuur, Alain F. and Ieno, Elena N. and Elphick, Chris S.},
  year = {2010},
  journal = {Methods in Ecology and Evolution},
  volume = {1},
  number = {1},
  pages = {3--14},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210X.2009.00001.x},
  urldate = {2021-01-25},
  abstract = {1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a random sample of their work (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniques employed. 2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially troublesome in applied ecology, where management and policy decisions are often at stake. 3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. 4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance of making wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses.},
  copyright = {{\copyright} 2009 The Authors. Journal compilation {\copyright} 2009 British Ecological Society},
  langid = {english},
  keywords = {collinearity,data exploration,independence,transformations,type I and II errors,zero inflation},
  file = {/home/julien/Documents/PDF/Zuur et al_2010_Methods in Ecology and Evolution.pdf;/home/julien/Documents/PDF/bibliobdd/zotero/storage/KEGXHN4S/j.2041-210X.2009.00001.html}
}
