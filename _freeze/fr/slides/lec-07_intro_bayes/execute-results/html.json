{
  "hash": "9c0be360402ad1ed539aa4387b00c6e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction Bayesian statistics\"\nsubtitle: Why make it simple when you could go Bayesian\nauthor: Julien Martin\ninstitute:  BIO 8940 - Lecture 7\ndate: today\nfrom: markdown+emoji\nformat:\n  revealjs: \n    width: 1600\n    height: 950\n    chalkboard: true\n    theme: [default]\n    css: [assets/theme_chalk/whiteboard-blue.css]\n#    output-location: column-fragment\n#    logo: assets/MAD_logo_small_rb.png\n    footer: BIO 8940 - Lecture 7\n    show-notes: false\n    output-ext: slides.html\n  html:\n    self-contained: true\n    number-sections: true\n    code-link: true\n    format-links: false\n    css: assets/css/notes.css\n    number-depth: 2\n    comments:\n      hypothesis: true\n    output-ext: notes.html\neditor:\n  render-on-save: false\n---\n\n\n\n\n## Overview ...\n\n\n\n\n\n\n\n\n\n* For many years – until recently – Bayesian ideas in statistics \nwere widely dismissed, often without much thought\n* Advocates of Bayes had to fight hard to be heard, leading to\nan ‘us against the world’ mentality – & predictable backlash\n* Today, debates tend be less acrimonious, and more tolerant\n\n# Bayes' Theorem\n\n## Formal\n\nBayes' theorem is the results in a conditional probability of two events:\n\n::: {.content-visible when-format=\"revealjs\"}\n$$\nP[A|B] = \\frac{P[A\\ \\&\\ B]}{P[B]} = \\frac{P[B|A]  P[A]}{P[B]}\n$$\n\n. . .\n\n:::\n\n\nThe conditional probability of A given B is the conditional probability ob B given A scaled by the relative probability of A compared to B.\n\n$$\n\\underbrace{P[A|B]}_\\text{posterior probability} = \\frac{\\overbrace{P[B|A]}^\\text{likelihood} \\cdot \\overbrace{P[A]}^\\text{prior probability}}{\\underbrace{P[B]}_\\text{marginal probability}}\n$$\n\n## Reframed for hypothesis\n\nBayes' theorem can be seen as the conditonal probability of a hypothesis given the data:\n$$\nP[H_0 | \\text{data}] = \\frac{\\overbrace{P[\\text{data}|H_0]}^\\text{likelihood} \\cdot  \\overbrace{P[H_0]}^\\text{prior}}{P[data]}\n$$\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\nOr it can be seen as\n$$\n\\underbrace{P[H_0 | \\text{data}]}_{\\text{what we want to know}} = \\frac{\\overbrace{P[\\text{data}|H_0]}^\\text{what frequentist do} \\cdot \\overbrace{P[H_0]}^\\text{what we have a hard time understanding}}{\\underbrace{P[data]}_\\text{what we happily ignore}}\n$$\n\n# Bayesian inference\n\n## Error type and false positive/negative\n\nHere we are counting number of observations in each case\n\n|Reality       |Reject H~0~ |Accept H~0~ |Total |\n|:-------------|:----------:|:----------:|:--------------:|\n|H~0~ is true |a (Type I error)           |b           | a + b |\n|H~0~ is false |c           |d (type II error)          | c + d |\n|Total         |a+c         |b+d         |N (number of obs)    |\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n[False positive]{.emph} $P[H_0 \\text{ true} | \\text{Reject }H_0] = \\frac{a}{a+c}$\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n[False negative]{.emph}: $P[H_0\\ false | Accept\\ H_0] = \\frac{d}{b+d}$\n\n## Error type and false positive/negative\n\nSame thing but with probabilities instead of number of observations\n\n|Reality       |Reject H~0~ $[\\not H_0]$|Accept H~0~ $[H_0]$ |Total |\n|:----------------|:----------:|:----------:|:------------:|\n|H~0~ true [H~0~^+^] | $P[H_0^+| \\not H_0] P[H_0^+]$ | $P[H_0^+| H_0] P[H_0^+]$ | $P[H_0^+]$ |\n|H~0~ false [H~0~^-^]| $P[H_0^-| \\not H_0] P[H_0^-]$ | $P[H_0^-| H_0] P[H_0^-]$ | $P[H_0^-]$|\n|Total         | $P[\\not H_0]$ | $P[H_0]$         | 1 |\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n[False positive]{.emph} $P[H_0 \\text{ true} | \\text{Reject }H_0]$\n$$\n\\begin{align}\nP[H_0^+ | \\not H_0] &=\\frac{P[\\not H_0 | H_0^+ ]  P[H_0^+ ]}{P[\\not H_0]} \\\\ \n&= \\frac{P[\\not H_0 | H_0^+ ]  P[H_0^+]}\n  {P[\\not H_0 | H_0^+]  P[H_0^+ ] + P[ \\not H_0 | H_0^- ]  P[H_0^-]} \n\\end{align}\n$$\n\n\n::: {.content-visible when-format=\"revealjs\"}\n## Applied to Covid\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nAssuming N test = 100,\n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | | | |\n|Has COVID | | | |\n|Total     | | |**100**|\n\n\n## Applied to Covid\n\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\nAssuming N test = 100 \n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | | | [99]{.blue} |\n|Has COVID | | | [1]{.blue} |\n|Total     | | | 100 |\n\n1. [Adding the prior]{.blue}\n\n\n## Applied to Covid\n\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\nAssuming N test = 100 \n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   |  |  | [99]{.blue} |\n|Has COVID |   [0.8]{.purple}  |   [0.2]{.purple}   | [1]{.blue} |\n|Total     |  | | 100 |\n\n\n1. [Adding the prior]{.blue}\n2. [Adding the Type II error]{.purple}\n\n## Applied to Covid\n\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\nAssuming N test = 100 \n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | [4.95]{.red} | [94.05]{.red} | [99]{.blue} |\n|Has COVID |   [0.8]{.purple}  |   [0.2]{.purple}   | [1]{.blue} |\n|Total     |  |  | 100|\n\n\n1. [Adding the prior]{.blue}\n2. [Adding the Type II error]{.purple}\n3. [Adding the Type I error]{.red}\n\n:::\n\n\n## Applied to Covid\n\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\nAssuming N test = 100 \n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | [4.95]{.red} | [94.05]{.red} | [99]{.blue} |\n|Has COVID |   [0.8]{.purple}  |   [0.2]{.purple}   | [1]{.blue} |\n|Total     | [5.75]{.orange} | [94.25]{.orange} | 100 |\n\n1. [Adding the prior]{.blue}\n2. [Adding the Type II error]{.purple}\n3. [Adding the Type I error]{.red}\n4. [Adding colum sums]{.orange}\n\n\n## Applied to Covid \n\n\nWhy does it matter? If 1% of a population have covid, for a \nscreening test with 80% sensitivity (1- Type II) and 95% specificity (1-Type I).\n\n\\\n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | 4.95 | 94.05 | 99 |\n|Has COVID |   0.8  |   0.2   | 1 |\n|Total     | 5.75 | 94.25 | 100 |\n\n::: {.incremental}\n\n* True positive: **P[ Covid | test + ] = 0.139**\n* True negative: **P[ Healthy | test - ] = 0.998**\n* False positive: P[ Healthy | test + ] = 0.861\n* False negative: P[ Covid | test - ] = 0.002\n:::\n\n:::{.notes}\nTalk about changing prior implications\n:::\n\n## What if COVID % changes?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWhy does it matter? If 20% of a population have covid instead of 1%?\n\n\\\n\n|Reality   |Test +ve |Test -ve |Total |\n|:---------|:-------:|:-------:|:----:|\n|Healthy   | 4 | 76 | 80 |\n|Has COVID |   16  |   4   | 20 |\n|Total     | 20 | 80 | 100 |\n\n* True positive: P[ Covid | test + ] = 0.8\n* True negative: P[ Healthy | test - ] = 0.95\n* False positive: P[ Healthy | test + ] = 0.2\n* False negative: P[ Covid | test - ] = 0.05\n\n\n:::{.notes}\nExplain impact of changing prior on test results reliability\n:::\n\n## Prosecutor's fallacy\n\n**Mixing up P[ A | B ] with P[ B | A ] is the Prosecutor’s Fallacy**\n\n::: {.center .emph}\nsmall P evidence given innocence $\\neq$ small P of innocence given evidence\n:::\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n**True Story**\n![](assets/img_l7/s_clark.jpg){.absolute top=0 right=50 width=\"250\" height=\"400\"}\n\n::: {.incremental}\n* After the sudden death of two baby sons, Sally Clark was sentenced to life in prison in 1999\n* Expert witness Prof Roy Meadow had interpreted the small probability of two cot deaths as a small probability of Clark’s innocence\n* After a long campaign, including refutation of Meadow’s\nstatistics (among other errors), Clark was cleared in 2003\n* After being freed, she developed alcoholism and died in 2007\n:::\n\n## Meeting mosquitoes\n\n![](assets/img_l7/sharks.png){fig-align=\"center\"}\n\n## Bayes' Theorem\n\nBayes’ Theorem is a rule about the ‘language’ of probabilities, that can be used in any analysis\ndescribing random variables, i.e. any data analysis.\n\n[Q. So why all the fuss?]{.emph}\n\nA. Bayesian inference uses more than just Bayes’ Theorem\n\nBayesian inference uses the ‘language’ of probability to describe what is known\nabout parameters.\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n::: {.callout-warning .large}\nFrequentist inference, e.g. using p-values & confidence intervals, does not quantify what is known about parameters. many people initially think it does; an important job for instructors of intro Stat/Biostat courses is convincing those people that they are wrong\n:::\n\n# Frequentist and Bayesian\n\n[A shooting cartoon]{.emph .center}\n\n\n::: {.small}\nAdapted from Gonick & Smith, The Cartoon Guide to Statistics\n:::\n\n---\n\n![](assets/img_l7/shoot_1.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_2.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_3.jpg){fig-align=\"center\"}\n\n::: {.notes}\nWe ‘trap’ the truth with 95% confidence. 95% of what?\n:::\n\n---\n\n![](assets/img_l7/shoot_4.jpg){fig-align=\"center\"}\n\n---\n\n## 95% of what?\n\n* We ‘trap’ the truth with 95% confidence.\n* 95% of what?\n* The interval traps the truth in 95% of experiments.\n\n::: {.emph}\nTo define\nanything frequentist, you have to imagine repeated experiments.\n:::\n\nLet’s do some more ‘target practice’, for frequentist testing\n\n---\n\n![](assets/img_l7/shoot_5.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_6.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_7.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_8.jpg){fig-align=\"center\"}\n\n---\n\n## Frequentist testing \n\n* imagine running your experiment again and again, so\n  * On day 1 you collect data and construct a [valid] 95% confidence interval for a parameter $\\theta_1$. \n  * On day 2 you collect new data and construct a 95% confidence interval for an unrelated parameter $\\theta_2$.\n  * On day 3 ... [the same]. and so on constructing confidence intervals each time\n* ... 95% of your intervals will trap the true parameter value\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n* ... it does not says anything about whether your data is\nin the 95% or the 5%\n* ... it requires you to think about many other datasets, not just the one you have to analyze\n\n::: {.content-visible when-format=\"revealjs\"}\n. . .\n:::\n\n[How does Bayesian inference differ? Let’s take aim...]{.emph}\n\n---\n\n![](assets/img_l7/shoot_9.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_10.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_11.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_12.jpg){fig-align=\"center\"}\n\n---\n\n![](assets/img_l7/shoot_13.jpg){fig-align=\"center\"}\n\n---\n\n## Here it is in practice\n\n::: {.incremental}\n* Air France Flight 447 crashed in the ocean On June 1, 2009.\n\n\\\n\n* Major wreckage recovered within 5 days. No blackbox\n![](assets/img_l7/plane_crash.jpg){.absolute top=50 right=50 width=\"400\" height=\"300\"}\n\n\\\n\n* Probability of blackbox location described via Bayesian inference\n![](assets/img_l7/plane_crash_map.jpg){.absolute top=500 right=50 width=\"400\" height=\"400\"}\n\n\\\n\n* Eventually, the black box was found in the red area\n:::\n\n# Bayesian inference\n\n## Updating knowledge\n\nWe use:\n\n* [**Prior distribution**]{.underline}: what you know about parameter β, excluding the information in the data – denoted $P_{prior}(β)$\n* [**Likelihood**]{.underline}: based on modeling assumptions, how [relatively] likely the data Y are if the truth is β - denoted $f(Y|β)$\n\nTo get a posterior distribution, denoted $P_{post}(β|Y)$: stating what we know about β combining the prior with the data – ?\n\nBayes Theorem used for inference tells us:\n\n$$\n\\begin{align}\nP_{post}(β|Y) &∝ f(Y|β) × P_{prior}(β)\\\\\n\\text{Posterior} &∝ \\text{Likelihood} × \\text{Prior}\n\\end{align}\n$$\n\n[... and that’s it! (essentially!)]{.emph}\n\n::: {.notes}\n* No replications – e.g. no replicate plane searches\n* Given modeling assumptions & prior, process is automatic\n* Keep adding data, and updating knowledge, as data becomes\navailable... knowledge will concentrate around true β\n:::\n\n::: {.content-visible when-format=\"revealjs\"}\n\n## Updating knowledge\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n## Updating knowledge\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n:::\n\n## Updating knowledge {auto-animate=\"true\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n## Updating knowledge {auto-animate=\"true\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n> A Bayesian is one who, vaguely **expecting a horse**, and catching a **glimpse of a donkey**, strongly **believes he has seen a mule**\n\n## Where do priors come from\n\nPriors come from all data external to the current study (*i.e.* everything else)\n‘Boiling down’ what subject-matter experts know/think is known as eliciting a prior.\nIt’s not easy but here are some simple tips\n\n* Discuss parameters experts understand – e.g. code variables so intercept is mean outcome in people with average covariates, not with age = height = ... = 0\n* Avoid leading questions (just as in survey design)\n* The ‘language’ of probability is unfamiliar, help users express their uncertainty\n\n## Where do priors come from\n\n::: {.center}\nUse stickers or a survey in the hallway\n:::\n\n:::: {.columns}\n\n::: {.column}\n\n![](assets/img_l7/priors_1.png)\n\nUse stickers (Johnson et\nal 2010, J Clin Epi) for survival when taking warfarin\n\n:::\n\n::: {.column}\n\n![](assets/img_l7/priors_2.png)\n\nNormalize marks (Latthe et al\n2005, J Obs Gync) for pain effect of LUNA vs placebo\n\n:::\n\n::::\n\n::: {.notes}\n* Ideas to help experts ‘translate’ to the language of probability\n* Typically these ‘coarse’ priors are smoothed. Providing the\nbasic shape remains, exactly how much you smooth is unlikely\nto be critical in practice.\n* Elicitation is also very useful for non-Bayesian analyses – it’s\nsimilar to study design & analysis planning\n:::\n\n## Where do priors come from\n\nIf the experts disagree? Try it both ways\n\n\\\n\nIf the posteriors differ, what you believe based on the data **depends** on your prior knowledge\n\n\\\n\n[To convince other people, expect to have to convince skeptics – and note that **convincing [rational] skeptics** is what science is all about]{.emph}\n\n# When priors don't matter (much)?\n\n## Very informative data\n\nWhen the data provide a lot more information than the prior\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\nPriors here are dominated by the likelihood, and they give very similar posteriors – i.e. everyone agrees. (Phew!)\n\n## Flat priors\n\nUsing very flat priors to represent ignorance\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\nFlat priors do [NOT]{.emph} actually represent ignorance!\n\n::: {.notes}\nMost of their support is for very extreme parameter values\n:::\n\n##  Bayesian $\\approx$ frequentist\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\nLikelihood gives the classic 95% confidence interval can be good approx of Bayesian 95% Highest Posterior Density interval\n\n## Bayesian $\\approx$ frequentist\n\nWith large samples (and some regularity conditions)\n\n* (sane) frequentist confidence intervals and (sane) Bayesian credible intervals are essentially identical\n\n* it’s actually okay to give Bayesian interpretations to 95% CIs,\ni.e. to say we have $\\neq$ 95% posterior belief that the true β lies within that range\n\n## Frequentist :smiley: & Bayesian :confused:\n\nPrior strongly supporting small effects, and with data from an imprecise study\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-07_intro_bayes_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n::: {.notes}\nFrequentist ‘Textbook’ analysis says ‘reject’ (p < 0.05, woohoo, Nature her we go)\n\nBayesian Posterior is ‘shrunk’ toward zero. We’re sure true β is very small (& hard to replicate) & we’re unsure of its sign. Wait a second, about that front page\n:::\n\n\n## Where is Bayesian approach used\n\n* Almost any analysis\n\n* Bayesian arguments are often seen in\n  \n  * Hierarchical modeling (Some expert calls the classic frequentist version a “statistical no-man’s land”)\n\n  * Complex models: for messy data, measurement error, multiple sources of data   fitting them is possible under Bayesian approaches, but perhaps still not easy\n\n# Summary\n\n## Bayesian statistics:\n\n* I barely scratched the surface\n\n* Is useful in many settings, and you should know about it\n\n* Is often not very different in practice from frequentist statistics. It is often helpful to think about analyses from both Bayesian and non-Bayesian points of view\n\n* Is not reserved for hard-core mathematicians, or computer scientists, or philosophers. If you find it helpful, use it.\n\n# Happy modelling {.unnumbered}\n\n![](assets/img_l5/unicorn.png){fig-align=\"center\"}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}